{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T04:33:57.212451Z",
     "start_time": "2025-09-30T04:33:54.185274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from outlines.models import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import transformers\n"
   ],
   "id": "d42f776961f8a676",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T04:39:38.281241Z",
     "start_time": "2025-09-30T04:39:38.276996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_anneal_attn_mask(seq_len, bsz, dtype, device, attn_mask_ratio):\n",
    "    mask = torch.full((seq_len, seq_len), 0, device=device)\n",
    "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 1)\n",
    "    causal_mask = mask.to(dtype)\n",
    "\n",
    "    random_mask = torch.bernoulli(torch.full((seq_len, seq_len), 0.0, device=device) + attn_mask_ratio)\n",
    "\n",
    "    anneal_mask = torch.logical_or(causal_mask, random_mask)\n",
    "    expanded_mask = anneal_mask[None, None, :, :].expand(bsz, 1, seq_len, seq_len)\n",
    "    inverted_mask = 1.0 - expanded_mask.to(dtype)\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n"
   ],
   "id": "6f8528ad4aa2ebbd",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T04:33:57.227675Z",
     "start_time": "2025-09-30T04:33:57.219430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# looks like Qwen using SDPA should be fine with an adapted attention mask, no\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from transformers import DynamicCache\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from cachetools import Cache\n",
    "from typing import Optional, __all__\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "def qwen_new_forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n",
    ") -> BaseModelOutputWithPast:\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "    if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "        raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "    if self.gradient_checkpointing and self.training and use_cache:\n",
    "        logger.warning_once(\n",
    "            \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
    "        )\n",
    "        use_cache = False\n",
    "\n",
    "    # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n",
    "    if not isinstance(past_key_values, (type(None), Cache)):\n",
    "        raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "    if use_cache and past_key_values is None:\n",
    "        past_key_values = DynamicCache()\n",
    "\n",
    "    if cache_position is None:\n",
    "        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "        cache_position = torch.arange(\n",
    "            past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "        )\n",
    "\n",
    "    if position_ids is None:\n",
    "        position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "    ## Add by DiffuLLaMA, adapting for 4d attention-mask.\n",
    "    if attention_mask is not None and len(attention_mask.shape) == 4:\n",
    "        causal_mask = attention_mask\n",
    "        print(\"logging....attention-mask for 4d\")\n",
    "    else:\n",
    "        causal_mask = self._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "    print(causal_mask)\n",
    "\n",
    "    # causal_mask = self._update_causal_mask(\n",
    "    #     attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "    # )\n",
    "\n",
    "    hidden_states = inputs_embeds\n",
    "\n",
    "    # create position embeddings to be shared across the decoder layers\n",
    "    position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "    # decoder layers\n",
    "    all_hidden_states = () if output_hidden_states else None\n",
    "    all_self_attns = () if output_attentions else None\n",
    "\n",
    "    for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            layer_outputs = self._gradient_checkpointing_func(\n",
    "                partial(decoder_layer.__call__, **flash_attn_kwargs),\n",
    "                hidden_states,\n",
    "                causal_mask,\n",
    "                position_ids,\n",
    "                past_key_values,\n",
    "                output_attentions,\n",
    "                use_cache,\n",
    "                cache_position,\n",
    "                position_embeddings,\n",
    "            )\n",
    "        else:\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_values,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "                **flash_attn_kwargs,\n",
    "            )\n",
    "\n",
    "        hidden_states = layer_outputs[0]\n",
    "\n",
    "        if output_attentions:\n",
    "            all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "    hidden_states = self.norm(hidden_states)\n",
    "\n",
    "    # add hidden states from the last decoder layer\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states += (hidden_states,)\n",
    "\n",
    "    return BaseModelOutputWithPast(\n",
    "        last_hidden_state=hidden_states,\n",
    "        past_key_values=past_key_values if use_cache else None,\n",
    "        hidden_states=all_hidden_states,\n",
    "        attentions=all_self_attns,\n",
    "    )\n"
   ],
   "id": "957aab8657358189",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T04:33:58.480802Z",
     "start_time": "2025-09-30T04:33:57.272881Z"
    }
   },
   "cell_type": "code",
   "source": "transformers.models.qwen3.modeling_qwen3.Qwen3Model.forward = qwen_new_forward",
   "id": "dee98ee1bb785d2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-30 14:03:57,542] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/miniconda3/envs/goedelv2/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/sean/miniconda3/envs/goedelv2/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T04:34:02.446547Z",
     "start_time": "2025-09-30T04:34:00.935200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"Goedel-LM/Goedel-Prover-V2-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, trust_remote_code=True)\n"
   ],
   "id": "2276d415d49ebe71",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 28.36it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T04:37:11.113742Z",
     "start_time": "2025-09-30T04:37:11.109113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "formal_statement = \"\"\"\n",
    "import Mathlib\n",
    "import Aesop\n",
    "\n",
    "set_option maxHeartbeats 0\n",
    "\n",
    "open BigOperators Real Nat Topology Rat\n",
    "\n",
    "\n",
    "theorem square_equation_solution {x y : ℝ} (h : x^2 + y^2 = 2*x - 4*y - 5) : x + y = -1 := by\n",
    "  sorry\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Complete the following Lean 4 code:\n",
    "\n",
    "```lean4\n",
    "{}```\n",
    "\n",
    "Before producing the Lean 4 code to formally prove the given theorem, provide a detailed proof plan outlining the main proof steps and strategies.\n",
    "The plan should highlight key ideas, intermediate lemmas, and proof structures that will guide the construction of the final formal proof.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat = [\n",
    "    [{\"role\": \"user\", \"content\": 'ABC'}],\n",
    "    [{'role': 'user', 'content': 'ABCDEF'}]\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, padding='longest',\n",
    "                                       return_dict=True, return_tensors='pt')"
   ],
   "id": "a259e83822c572f0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "inputs",
   "id": "deae7014753ac5e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "inputs['attention_mask']",
   "id": "26c5b494841bdc43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.forward(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
   "id": "204419cb20de91d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T04:41:43.822962Z",
     "start_time": "2025-09-30T04:41:43.810284Z"
    }
   },
   "cell_type": "code",
   "source": "attn_mask = get_anneal_attn_mask(inputs['input_ids'].shape[1], inputs['input_ids'].shape[0], dtype=torch.bfloat16, device=inputs['input_ids'].device, attn_mask_ratio=0.5)",
   "id": "b5efd98a9cd53245",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "logits = model.forward(inputs['input_ids'], attention_mask=attn_mask, output_attentions=True)\n",
   "id": "8ea2f2f31dc09932"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print (logits.attentions[0][0][1])\n",
    "# assert attentions are 0 at attn_mask, and nonzero elsewhere\n",
    "\n",
    "new_mask = attn_mask.expand(-1, logits.attentions[0].shape[1], -1,-1)\n",
    "\n",
    "for layer in logits.attentions:\n",
    "    for x_ in layer:\n",
    "        for attns in x_:\n",
    "            print (attns)\n",
    "            break\n",
    "\n",
    "# comparing to attn_mask, looks like the attentions are correctly working. I.e. attentions are 0 for masked, and non-zero elsewhere\n",
    "# Interestingly, high attention values often found for subsequent tokens even though model only used to seeing past tokens.\n",
    "# Suggests annealing worthwhile to limit influence earlier in adaptation process"
   ],
   "id": "28021149fd27e00c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# todo:\n",
    "# - set up dataset, test edit flow data prep code (couplings, alignments etc. ).\n",
    "# - Look at generating from empty coupling as well as error correcting coupling (i.e. add error to context, modify the code directly)\n",
    "# - Set up model wrapper (rate predictions etc.), Quantization/LoRA. Unlike MDM adaptation, we can probably keep the logits unshifted, as we are predicting both substitute and insert next token probs, as well as delete.\n"
   ],
   "id": "f9e2ddcf8ad3a982"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
