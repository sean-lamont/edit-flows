{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:06:59.364206456Z",
     "start_time": "2025-10-16T02:06:54.285282454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from outlines.models import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn.attention.flex_attention import create_block_mask, and_masks, or_masks, create_mask\n"
   ],
   "id": "d42f776961f8a676",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/goedelv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:06:59.741910264Z",
     "start_time": "2025-10-16T02:06:59.735473731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_anneal_attn_mask(seq_len, bsz, dtype, device, attn_mask_ratio):\n",
    "    mask = torch.full((seq_len, seq_len), 0, device=device)\n",
    "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 1)\n",
    "    causal_mask = mask.to(dtype)\n",
    "\n",
    "    random_mask = torch.bernoulli(torch.full((seq_len, seq_len), 0.0, device=device) + attn_mask_ratio)\n",
    "\n",
    "    anneal_mask = torch.logical_or(causal_mask, random_mask)\n",
    "    expanded_mask = anneal_mask[None, None, :, :].expand(bsz, 1, seq_len, seq_len)\n",
    "    inverted_mask = 1.0 - expanded_mask.to(dtype)\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "    return anneal_mask\n",
    "\n"
   ],
   "id": "6f8528ad4aa2ebbd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:06:59.748743012Z",
     "start_time": "2025-10-16T02:06:59.743209755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initial mask is bsz x seq_len, want bsz x seq_len x seq_len\n",
    "def get_anneal_mask_new(mask, attn_mask_ratio):\n",
    "\n",
    "    random_mask = torch.bernoulli(torch.full((seq_len, seq_len), 0.0, device=device) + attn_mask_ratio)"
   ],
   "id": "f798ac8626a9f031",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:00.161574553Z",
     "start_time": "2025-10-16T02:06:59.749482268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# looks like Qwen using SDPA should be fine with an adapted attention mask, no\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from transformers import DynamicCache\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask\n",
    "from cachetools import Cache\n",
    "from typing import Optional, __all__\n",
    "from transformers.utils import logging, TransformersKwargs\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "# def qwen_new_forward(\n",
    "#         self,\n",
    "#         input_ids: Optional[torch.LongTensor] = None,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         position_ids: Optional[torch.LongTensor] = None,\n",
    "#         past_key_values: Optional[Cache] = None,\n",
    "#         inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "#         use_cache: Optional[bool] = None,\n",
    "#         output_attentions: Optional[bool] = None,\n",
    "#         output_hidden_states: Optional[bool] = None,\n",
    "#         cache_position: Optional[torch.LongTensor] = None,\n",
    "#         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n",
    "# ) -> BaseModelOutputWithPast:\n",
    "#\n",
    "#     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "#     output_hidden_states = (\n",
    "#         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "#     )\n",
    "#     use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "#\n",
    "#     if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "#         raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "#\n",
    "#     if self.gradient_checkpointing and self.training and use_cache:\n",
    "#         logger.warning_once(\n",
    "#             \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
    "#         )\n",
    "#         use_cache = False\n",
    "#\n",
    "#     # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n",
    "#     if not isinstance(past_key_values, (type(None), Cache)):\n",
    "#         raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n",
    "#\n",
    "#     if inputs_embeds is None:\n",
    "#         inputs_embeds = self.embed_tokens(input_ids)\n",
    "#\n",
    "#     if use_cache and past_key_values is None:\n",
    "#         past_key_values = DynamicCache()\n",
    "#\n",
    "#     if cache_position is None:\n",
    "#         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "#         cache_position = torch.arange(\n",
    "#             past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "#         )\n",
    "#\n",
    "#     if position_ids is None:\n",
    "#         position_ids = cache_position.unsqueeze(0)\n",
    "#\n",
    "#     ## Add by DiffuLLaMA, adapting for 4d attention-mask.\n",
    "#     if attention_mask is not None and len(attention_mask.shape) == 4:\n",
    "#         causal_mask = attention_mask\n",
    "#         print(\"logging....attention-mask for 4d\")\n",
    "#     else:\n",
    "#         causal_mask = self._update_causal_mask(\n",
    "#             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "#         )\n",
    "#\n",
    "#     print(causal_mask)\n",
    "#\n",
    "#     # causal_mask = self._update_causal_mask(\n",
    "#     #     attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "#     # )\n",
    "#\n",
    "#     hidden_states = inputs_embeds\n",
    "#\n",
    "#     # create position embeddings to be shared across the decoder layers\n",
    "#     position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "#\n",
    "#     # decoder layers\n",
    "#     all_hidden_states = () if output_hidden_states else None\n",
    "#     all_self_attns = () if output_attentions else None\n",
    "#\n",
    "#     for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
    "#         if output_hidden_states:\n",
    "#             all_hidden_states += (hidden_states,)\n",
    "#\n",
    "#         if self.gradient_checkpointing and self.training:\n",
    "#             layer_outputs = self._gradient_checkpointing_func(\n",
    "#                 partial(decoder_layer.__call__, **flash_attn_kwargs),\n",
    "#                 hidden_states,\n",
    "#                 causal_mask,\n",
    "#                 position_ids,\n",
    "#                 past_key_values,\n",
    "#                 output_attentions,\n",
    "#                 use_cache,\n",
    "#                 cache_position,\n",
    "#                 position_embeddings,\n",
    "#             )\n",
    "#         else:\n",
    "#             layer_outputs = decoder_layer(\n",
    "#                 hidden_states,\n",
    "#                 attention_mask=causal_mask,\n",
    "#                 position_ids=position_ids,\n",
    "#                 past_key_value=past_key_values,\n",
    "#                 output_attentions=output_attentions,\n",
    "#                 use_cache=use_cache,\n",
    "#                 cache_position=cache_position,\n",
    "#                 position_embeddings=position_embeddings,\n",
    "#                 **flash_attn_kwargs,\n",
    "#             )\n",
    "#\n",
    "#         hidden_states = layer_outputs[0]\n",
    "#\n",
    "#         if output_attentions:\n",
    "#             all_self_attns += (layer_outputs[1],)\n",
    "#\n",
    "#     hidden_states = self.norm(hidden_states)\n",
    "#\n",
    "#     # add hidden states from the last decoder layer\n",
    "#     if output_hidden_states:\n",
    "#         all_hidden_states += (hidden_states,)\n",
    "#\n",
    "#     return BaseModelOutputWithPast(\n",
    "#         last_hidden_state=hidden_states,\n",
    "#         past_key_values=past_key_values if use_cache else None,\n",
    "#         hidden_states=all_hidden_states,\n",
    "#         attentions=all_self_attns,\n",
    "#     )\n",
    "#"
   ],
   "id": "957aab8657358189",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:00.171937470Z",
     "start_time": "2025-10-16T02:07:00.162500031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def qwen_new_forward(\n",
    "    self,\n",
    "    input_ids: Optional[torch.LongTensor] = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[Cache] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    **kwargs: Unpack[TransformersKwargs],\n",
    ") -> BaseModelOutputWithPast:\n",
    "    if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "        raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "    if use_cache and past_key_values is None:\n",
    "        past_key_values = DynamicCache(config=self.config)\n",
    "\n",
    "    if cache_position is None:\n",
    "        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "        cache_position = torch.arange(\n",
    "            past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "        )\n",
    "\n",
    "    if position_ids is None:\n",
    "        position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "    # It may already have been prepared by e.g. `generate`\n",
    "\n",
    "    # if not isinstance(causal_mask_mapping := attention_mask, dict):\n",
    "\n",
    "    print (self.has_sliding_layers)\n",
    "    print (self.config._attn_implementation)\n",
    "\n",
    "    # if len(attention_mask.shape) == 4:\n",
    "    #     causal_mask_mapping = {'full_attention': attention_mask}\n",
    "    #     print (attention_mask)\n",
    "    #\n",
    "    if not isinstance(causal_mask_mapping := attention_mask, dict):\n",
    "\n",
    "        # Prepare mask arguments\n",
    "        mask_kwargs = {\n",
    "            \"config\": self.config,\n",
    "            \"input_embeds\": inputs_embeds,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"cache_position\": cache_position,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"position_ids\": position_ids,\n",
    "        }\n",
    "        # Create the masks\n",
    "        causal_mask_mapping = {\n",
    "            \"full_attention\": create_causal_mask(**mask_kwargs),\n",
    "        }\n",
    "        # The sliding window alternating layers are not always activated depending on the config\n",
    "        if self.has_sliding_layers:\n",
    "            causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n",
    "\n",
    "    print (causal_mask_mapping.items())\n",
    "\n",
    "    hidden_states = inputs_embeds\n",
    "\n",
    "    # create position embeddings to be shared across the decoder layers\n",
    "    position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "    for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
    "        hidden_states = decoder_layer(\n",
    "            hidden_states,\n",
    "            attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            cache_position=cache_position,\n",
    "            position_embeddings=position_embeddings,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    hidden_states = self.norm(hidden_states)\n",
    "    return BaseModelOutputWithPast(\n",
    "        last_hidden_state=hidden_states,\n",
    "            past_key_values=past_key_values if use_cache else None,\n",
    "        )\n"
   ],
   "id": "dee98ee1bb785d2b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:00.177810389Z",
     "start_time": "2025-10-16T02:07:00.172562945Z"
    }
   },
   "cell_type": "code",
   "source": "# transformers.models.qwen3.modeling_qwen3.Qwen3Model.forward = qwen_new_forward",
   "id": "2276d415d49ebe71",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:00.664552689Z",
     "start_time": "2025-10-16T02:07:00.178780307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"Goedel-LM/Goedel-Prover-V2-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, trust_remote_code=True, _attn_implementation='flex_attention')\n"
   ],
   "id": "a259e83822c572f0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 110.97it/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:00.692080199Z",
     "start_time": "2025-10-16T02:07:00.665424496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "formal_statement = \"\"\"\n",
    "import Mathlib\n",
    "import Aesop\n",
    "\n",
    "set_option maxHeartbeats 0\n",
    "\n",
    "open BigOperators Real Nat Topology Rat\n",
    "\n",
    "\n",
    "theorem square_equation_solution {x y : ℝ} (h : x^2 + y^2 = 2*x - 4*y - 5) : x + y = -1 := by\n",
    "  sorry\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Complete the following Lean 4 code:\n",
    "\n",
    "```lean4\n",
    "{}```\n",
    "\n",
    "Before producing the Lean 4 code to formally prove the given theorem, provide a detailed proof plan outlining the main proof steps and strategies.\n",
    "The plan should highlight key ideas, intermediate lemmas, and proof structures that will guide the construction of the final formal proof.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat = [\n",
    "    [{\"role\": \"user\", \"content\": 'ABC' * 70}],\n",
    "    [{'role': 'user', 'content': 'ABCDEF' * 70}]\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, padding='longest',\n",
    "                                       return_dict=True, return_tensors='pt')"
   ],
   "id": "deae7014753ac5e2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:00.702334084Z",
     "start_time": "2025-10-16T02:07:00.692995326Z"
    }
   },
   "cell_type": "code",
   "source": "inputs",
   "id": "26c5b494841bdc43",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,    872,    198,  25411,  25411,  25411,  25411,  25411,  25411,\n",
       "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
       "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
       "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
       "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
       "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
       "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
       "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
       "          25411, 151645,    198, 151644,  77091,    198, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643],\n",
       "        [151644,    872,    198,  25411,  13649,  25411,  13649,  25411,  13649,\n",
       "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
       "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
       "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
       "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
       "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
       "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
       "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
       "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
       "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
       "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
       "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
       "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
       "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
       "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
       "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649, 151645,\n",
       "            198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:00.711959815Z",
     "start_time": "2025-10-16T02:07:00.703093691Z"
    }
   },
   "cell_type": "code",
   "source": "inputs['attention_mask']",
   "id": "204419cb20de91d6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:00.717973315Z",
     "start_time": "2025-10-16T02:07:00.712969563Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "14b79d6cb21b7e17",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:03.160560367Z",
     "start_time": "2025-10-16T02:07:00.718536429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def causal_mask(b,h,q_idx,kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "def create_padding_mask(pads):\n",
    "    def padding(b, h, q_idx, kv_idx):\n",
    "        return ~pads[b, q_idx] & ~pads[b, kv_idx]\n",
    "    return padding\n",
    "\n",
    "\n",
    "attn_mask_ratio = 0.5\n",
    "seq_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "random_mask = (torch.rand(seq_len, seq_len) < attn_mask_ratio).to('cuda')\n",
    "pads = inputs['attention_mask'].to('cuda')\n",
    "padding_mask = create_padding_mask(pads)\n",
    "\n",
    "def create_random_mask(b,h,q_idx,kv_idx):\n",
    "    return random_mask[q_idx][kv_idx]\n",
    "\n",
    "\n",
    "# precompute block mask for after attention annealing, for attention annealing implement in forward method\n",
    "\n",
    "# full_mask = and_masks(*[or_masks(*[causal_mask, create_random_mask]), padding_mask])\n",
    "full_mask = or_masks(*[causal_mask, create_random_mask])\n",
    "\n",
    "\n",
    "# block_mask = create_block_mask(full_mask, inputs['input_ids'].shape[0], None, len(inputs['input_ids'][0]), len(inputs['input_ids'][0]), device='cuda')\n",
    "block_mask = create_block_mask(causal_mask, None, None, len(inputs['input_ids'][0]), len(inputs['input_ids'][0]), device='cuda', _compile=True)\n",
    "\n",
    "\n",
    "\n",
    "block_mask_2 = create_block_mask(causal_mask, None, None, len(inputs['input_ids'][0]), len(inputs['input_ids'][0]), device='cuda')\n"
   ],
   "id": "e557c902cdcb7e76",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:03.446556798Z",
     "start_time": "2025-10-16T02:07:03.436584535Z"
    }
   },
   "cell_type": "code",
   "source": "test_mask = create_mask(full_mask, None, None, len(inputs['input_ids'][0]), len(inputs['input_ids'][0]), device='cuda')\n",
   "id": "d4d1a27f966ea6f3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:03.454998779Z",
     "start_time": "2025-10-16T02:07:03.447483416Z"
    }
   },
   "cell_type": "code",
   "source": "test_mask",
   "id": "8774adf7181e24fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True, False,  True,  ..., False,  True, False],\n",
       "          [ True,  True, False,  ..., False, False,  True],\n",
       "          [ True,  True,  True,  ..., False, False,  True],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]]]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:03.461862216Z",
     "start_time": "2025-10-16T02:07:03.455691115Z"
    }
   },
   "cell_type": "code",
   "source": "block_mask\n",
   "id": "5d7c3fa731ed1812",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockMask(\n",
       "    kv_num_blocks=torch.Size([1, 1, 2]),\n",
       "    kv_indices=torch.Size([1, 1, 2, 2]),\n",
       "    full_kv_num_blocks=torch.Size([1, 1, 2]),\n",
       "    full_kv_indices=torch.Size([1, 1, 2, 2]),\n",
       "    q_num_blocks=torch.Size([1, 1, 2]),\n",
       "    q_indices=torch.Size([1, 1, 2, 2]),\n",
       "    full_q_num_blocks=torch.Size([1, 1, 2]),\n",
       "    full_q_indices=torch.Size([1, 1, 2, 2]),\n",
       "    BLOCK_SIZE=(128, 128),\n",
       "    shape=(1, 1, 148, 148),\n",
       "    sparsity=-124.40%,\n",
       "    mask_mod=causal_mask\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:03.466654146Z",
     "start_time": "2025-10-16T02:07:03.462535552Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "48d218021a1c2ddb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:03.472298384Z",
     "start_time": "2025-10-16T02:07:03.467279051Z"
    }
   },
   "cell_type": "code",
   "source": "print (block_mask_2.mask_mod)",
   "id": "923ac1d8ad3f24f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function causal_mask at 0x7f7e997b0dc0>\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:03.478933389Z",
     "start_time": "2025-10-16T02:07:03.473089740Z"
    }
   },
   "cell_type": "code",
   "source": "block_mask",
   "id": "9be38473bc686d25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockMask(\n",
       "    kv_num_blocks=torch.Size([1, 1, 2]),\n",
       "    kv_indices=torch.Size([1, 1, 2, 2]),\n",
       "    full_kv_num_blocks=torch.Size([1, 1, 2]),\n",
       "    full_kv_indices=torch.Size([1, 1, 2, 2]),\n",
       "    q_num_blocks=torch.Size([1, 1, 2]),\n",
       "    q_indices=torch.Size([1, 1, 2, 2]),\n",
       "    full_q_num_blocks=torch.Size([1, 1, 2]),\n",
       "    full_q_indices=torch.Size([1, 1, 2, 2]),\n",
       "    BLOCK_SIZE=(128, 128),\n",
       "    shape=(1, 1, 148, 148),\n",
       "    sparsity=-124.40%,\n",
       "    mask_mod=causal_mask\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:03.485213331Z",
     "start_time": "2025-10-16T02:07:03.479587245Z"
    }
   },
   "cell_type": "code",
   "source": "inputs['input_ids'].shape",
   "id": "5c1b920d4bca22c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 148])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:06.438922827Z",
     "start_time": "2025-10-16T02:07:03.486021198Z"
    }
   },
   "cell_type": "code",
   "source": "model.cuda()",
   "id": "764f7fea44aeab07",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:06.536181481Z",
     "start_time": "2025-10-16T02:07:06.530233581Z"
    }
   },
   "cell_type": "code",
   "source": "# torch.compile(model)",
   "id": "67c7896b1ec80eac",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:06.544260238Z",
     "start_time": "2025-10-16T02:07:06.537103058Z"
    }
   },
   "cell_type": "code",
   "source": "print (inputs)",
   "id": "7ee5b78d32bdf89c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,    872,    198,  25411,  25411,  25411,  25411,  25411,  25411,\n",
      "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
      "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
      "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
      "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
      "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
      "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
      "          25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,  25411,\n",
      "          25411, 151645,    198, 151644,  77091,    198, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643],\n",
      "        [151644,    872,    198,  25411,  13649,  25411,  13649,  25411,  13649,\n",
      "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
      "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
      "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
      "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
      "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
      "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
      "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
      "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
      "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
      "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
      "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
      "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
      "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,\n",
      "          13649,  25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649,\n",
      "          25411,  13649,  25411,  13649,  25411,  13649,  25411,  13649, 151645,\n",
      "            198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:07:11.175762862Z",
     "start_time": "2025-10-16T02:07:06.545038464Z"
    }
   },
   "cell_type": "code",
   "source": "out = model.forward(inputs['input_ids'].to('cuda'), attention_mask=block_mask, output_hidden_states=True, output_attentions=True)\n",
   "id": "e77783564a8837a7",
   "outputs": [
    {
     "ename": "InductorError",
     "evalue": "RuntimeError: No valid triton configs. OutOfMemoryError: out of resource: triton_tem_fused_0 Required: 107008 Hardware limit:101376 Reducing block sizes or `num_stages` may help.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInductorError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblock_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001B[0m, in \u001B[0;36mcan_return_tuple.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_dict_passed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    917\u001B[0m     return_dict \u001B[38;5;241m=\u001B[39m return_dict_passed\n\u001B[0;32m--> 918\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    920\u001B[0m     output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_tuple()\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:480\u001B[0m, in \u001B[0;36mQwen3ForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;129m@can_return_tuple\u001B[39m\n\u001B[1;32m    444\u001B[0m \u001B[38;5;129m@auto_docstring\u001B[39m\n\u001B[1;32m    445\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Unpack[TransformersKwargs],\n\u001B[1;32m    457\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m CausalLMOutputWithPast:\n\u001B[1;32m    458\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;124;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;124;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001B[39;00m\n\u001B[1;32m    479\u001B[0m \u001B[38;5;124;03m    ```\"\"\"\u001B[39;00m\n\u001B[0;32m--> 480\u001B[0m     outputs: BaseModelOutputWithPast \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    481\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    485\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    486\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    491\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state\n\u001B[1;32m    492\u001B[0m     \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/utils/generic.py:1064\u001B[0m, in \u001B[0;36mcheck_model_inputs.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1061\u001B[0m                 monkey_patched_layers\u001B[38;5;241m.\u001B[39mappend((module, original_forward))\n\u001B[1;32m   1063\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1064\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1065\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m original_exception:\n\u001B[1;32m   1066\u001B[0m     \u001B[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001B[39;00m\n\u001B[1;32m   1067\u001B[0m     \u001B[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001B[39;00m\n\u001B[1;32m   1068\u001B[0m     \u001B[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001B[39;00m\n\u001B[1;32m   1069\u001B[0m     kwargs_without_recordable \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m recordable_keys}\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:410\u001B[0m, in \u001B[0;36mQwen3Model.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001B[0m\n\u001B[1;32m    407\u001B[0m position_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrotary_emb(hidden_states, position_ids)\n\u001B[1;32m    409\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m decoder_layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers]:\n\u001B[0;32m--> 410\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask_mapping\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdecoder_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    419\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    421\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(hidden_states)\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m BaseModelOutputWithPast(\n\u001B[1;32m    423\u001B[0m     last_hidden_state\u001B[38;5;241m=\u001B[39mhidden_states,\n\u001B[1;32m    424\u001B[0m     past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    425\u001B[0m )\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001B[0m, in \u001B[0;36mGradientCheckpointingLayer.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     91\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning_once(message)\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs), \u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m---> 94\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/utils/generic.py:1023\u001B[0m, in \u001B[0;36mcheck_model_inputs.<locals>.wrapper.<locals>.make_capture_wrapper.<locals>.wrapped_forward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1021\u001B[0m         output \u001B[38;5;241m=\u001B[39m orig_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1022\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1023\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43morig_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m   1025\u001B[0m     collected_outputs[key] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (output,)\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:260\u001B[0m, in \u001B[0;36mQwen3DecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    258\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_layernorm(hidden_states)\n\u001B[1;32m    259\u001B[0m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[0;32m--> 260\u001B[0m hidden_states, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    264\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    266\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    270\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    272\u001B[0m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/utils/generic.py:1023\u001B[0m, in \u001B[0;36mcheck_model_inputs.<locals>.wrapper.<locals>.make_capture_wrapper.<locals>.wrapped_forward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1021\u001B[0m         output \u001B[38;5;241m=\u001B[39m orig_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1022\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1023\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43morig_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m   1025\u001B[0m     collected_outputs[key] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (output,)\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:216\u001B[0m, in \u001B[0;36mQwen3Attention.forward\u001B[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meager\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    214\u001B[0m     attention_interface \u001B[38;5;241m=\u001B[39m ALL_ATTENTION_FUNCTIONS[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_attn_implementation]\n\u001B[0;32m--> 216\u001B[0m attn_output, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[43mattention_interface\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkey_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalue_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention_dropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscaling\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m    \u001B[49m\u001B[43msliding_window\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msliding_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# diff with Llama\u001B[39;49;00m\n\u001B[1;32m    225\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    228\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m*\u001B[39minput_shape, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m    229\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mo_proj(attn_output)\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/integrations/flex_attention.py:298\u001B[0m, in \u001B[0;36mflex_attention_forward\u001B[0;34m(module, query, key, value, attention_mask, scaling, softcap, head_mask, s_aux, **kwargs)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_lse \u001B[38;5;129;01mand\u001B[39;00m s_aux \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    295\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttention sinks cannot be run on CPU with flex attention. Please switch to a different device, e.g. CUDA\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    296\u001B[0m     )\n\u001B[0;32m--> 298\u001B[0m flex_attention_output \u001B[38;5;241m=\u001B[39m \u001B[43mcompile_friendly_flex_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    299\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    300\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    301\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    302\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscore_mod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscore_mod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    303\u001B[0m \u001B[43m    \u001B[49m\u001B[43mblock_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblock_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    304\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_gqa\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable_gqa\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    306\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkernel_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkernel_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Last time checked on PyTorch == 2.5.1: Flex Attention always computes the lse regardless.\u001B[39;49;00m\n\u001B[1;32m    308\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# For simplification, we thus always return it as no additional computations are introduced.\u001B[39;49;00m\n\u001B[1;32m    309\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_lse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_lse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;66;03m# lse is returned in float32\u001B[39;00m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_lse:\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/transformers/integrations/flex_attention.py:97\u001B[0m, in \u001B[0;36mcompile_friendly_flex_attention\u001B[0;34m(query, key, value, training, **kwargs)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcompile_friendly_flex_attention\u001B[39m(\n\u001B[1;32m     88\u001B[0m     query: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m     89\u001B[0m     key: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;66;03m# First call initialise singleton wrapper object, second call invokes the object method to return compiled flex attention\u001B[39;00m\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;66;03m# Do not use compiled version if already compiling forward (it raises issues)\u001B[39;00m\n\u001B[1;32m     96\u001B[0m     flex_attention_compiled \u001B[38;5;241m=\u001B[39m WrappedFlexAttention(training)() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling() \u001B[38;5;28;01melse\u001B[39;00m flex_attention\n\u001B[0;32m---> 97\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mflex_attention_compiled\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:749\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    745\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__cause__\u001B[39;00m  \u001B[38;5;66;03m# User compiler error\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ShortenTraceback \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    747\u001B[0m     \u001B[38;5;66;03m# Failures in the backend likely don't have useful\u001B[39;00m\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001B[39;00m\n\u001B[0;32m--> 749\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mremove_dynamo_frames() \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001B[39;00m\n\u001B[1;32m    750\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    751\u001B[0m     \u001B[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001B[39;00m\n\u001B[1;32m    752\u001B[0m     set_eval_frame(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:923\u001B[0m, in \u001B[0;36m_compile_fx_inner\u001B[0;34m(gm, example_inputs, **graph_kwargs)\u001B[0m\n\u001B[1;32m    921\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    922\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 923\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InductorError(e, currentframe())\u001B[38;5;241m.\u001B[39mwith_traceback(\n\u001B[1;32m    924\u001B[0m         e\u001B[38;5;241m.\u001B[39m__traceback__\n\u001B[1;32m    925\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    926\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    927\u001B[0m     TritonBundler\u001B[38;5;241m.\u001B[39mend_compile()\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:907\u001B[0m, in \u001B[0;36m_compile_fx_inner\u001B[0;34m(gm, example_inputs, **graph_kwargs)\u001B[0m\n\u001B[1;32m    905\u001B[0m TritonBundler\u001B[38;5;241m.\u001B[39mbegin_compile()\n\u001B[1;32m    906\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 907\u001B[0m     mb_compiled_graph \u001B[38;5;241m=\u001B[39m \u001B[43mfx_codegen_and_compile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    908\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_to_check\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgraph_kwargs\u001B[49m\n\u001B[1;32m    909\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    910\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m mb_compiled_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    911\u001B[0m     mb_compiled_graph\u001B[38;5;241m.\u001B[39m_time_taken_ns \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime_ns() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:1578\u001B[0m, in \u001B[0;36mfx_codegen_and_compile\u001B[0;34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001B[0m\n\u001B[1;32m   1573\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(scheme, _OutOfProcessFxCompile), (\n\u001B[1;32m   1574\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masync is only valid with an out-of-process compile mode\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1575\u001B[0m     )\n\u001B[1;32m   1576\u001B[0m     scheme \u001B[38;5;241m=\u001B[39m _AsyncFxCompile(scheme)\n\u001B[0;32m-> 1578\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mscheme\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcodegen_and_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_to_check\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:1456\u001B[0m, in \u001B[0;36m_InProcessFxCompile.codegen_and_compile\u001B[0;34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001B[0m\n\u001B[1;32m   1438\u001B[0m         compiled_fn \u001B[38;5;241m=\u001B[39m AotCodeCompiler\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[1;32m   1439\u001B[0m             graph,\n\u001B[1;32m   1440\u001B[0m             wrapper_code\u001B[38;5;241m.\u001B[39mvalue,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1453\u001B[0m             ],\n\u001B[1;32m   1454\u001B[0m         )\n\u001B[1;32m   1455\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1456\u001B[0m     compiled_module \u001B[38;5;241m=\u001B[39m \u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_to_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1457\u001B[0m     compiled_fn \u001B[38;5;241m=\u001B[39m compiled_module\u001B[38;5;241m.\u001B[39mcall\n\u001B[1;32m   1458\u001B[0m     compiled_fn_runner \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\n\u001B[1;32m   1459\u001B[0m         compiled_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrunner\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1460\u001B[0m     )\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/graph.py:2293\u001B[0m, in \u001B[0;36mGraphLowering.compile_to_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2286\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcompile_to_module\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m CompiledModule:\n\u001B[1;32m   2287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m dynamo_timed(\n\u001B[1;32m   2288\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGraphLowering.compile_to_module\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2289\u001B[0m         phase_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode_gen\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2290\u001B[0m         log_pt2_compile_event\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   2291\u001B[0m         dynamo_compile_column_us\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minductor_code_gen_cumulative_compile_time_us\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2292\u001B[0m     ):\n\u001B[0;32m-> 2293\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compile_to_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/graph.py:2303\u001B[0m, in \u001B[0;36mGraphLowering._compile_to_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2298\u001B[0m wrapper_code, _ \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2299\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcodegen_with_cpp_wrapper() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcpp_wrapper \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcodegen()\n\u001B[1;32m   2300\u001B[0m )\n\u001B[1;32m   2302\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(wrapper_code, ValueWithLineMap):\n\u001B[0;32m-> 2303\u001B[0m     mod \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compile_to_module_lines\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrapper_code\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2304\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(wrapper_code, FileBackedGraphModule):\n\u001B[1;32m   2305\u001B[0m     mod \u001B[38;5;241m=\u001B[39m wrapper_code\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/graph.py:2371\u001B[0m, in \u001B[0;36mGraphLowering._compile_to_module_lines\u001B[0;34m(self, wrapper_code)\u001B[0m\n\u001B[1;32m   2365\u001B[0m     trace_structured(\n\u001B[1;32m   2366\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minductor_output_code\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2367\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m\"\u001B[39m: path},\n\u001B[1;32m   2368\u001B[0m         payload_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m: wrapper_code\u001B[38;5;241m.\u001B[39mvalue,\n\u001B[1;32m   2369\u001B[0m     )\n\u001B[1;32m   2370\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m dynamo_timed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPyCodeCache.load_by_key_path\u001B[39m\u001B[38;5;124m\"\u001B[39m, log_pt2_compile_event\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m-> 2371\u001B[0m     mod \u001B[38;5;241m=\u001B[39m \u001B[43mPyCodeCache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_by_key_path\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2372\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2373\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2374\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlinemap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlinemap\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   2375\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconstants\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtorchbind_constants\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2376\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_key \u001B[38;5;241m=\u001B[39m key\n\u001B[1;32m   2378\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_path \u001B[38;5;241m=\u001B[39m path\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/codecache.py:3296\u001B[0m, in \u001B[0;36mPyCodeCache.load_by_key_path\u001B[0;34m(cls, key, path, linemap, attrs)\u001B[0m\n\u001B[1;32m   3293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mmodules_no_attr[path]\n\u001B[1;32m   3295\u001B[0m in_toplevel \u001B[38;5;241m=\u001B[39m in_toplevel_process()\n\u001B[0;32m-> 3296\u001B[0m mod \u001B[38;5;241m=\u001B[39m \u001B[43m_reload_python_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_sys_modules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43min_toplevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3298\u001B[0m \u001B[38;5;66;03m# unzip into separate lines/nodes lists\u001B[39;00m\n\u001B[1;32m   3299\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m in_toplevel:\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py:31\u001B[0m, in \u001B[0;36m_reload_python_module\u001B[0;34m(key, path, set_sys_modules)\u001B[0m\n\u001B[1;32m     29\u001B[0m mod\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__file__\u001B[39m \u001B[38;5;241m=\u001B[39m path\n\u001B[1;32m     30\u001B[0m mod\u001B[38;5;241m.\u001B[39mkey \u001B[38;5;241m=\u001B[39m key  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m \u001B[43mexec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__dict__\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__dict__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m set_sys_modules:\n\u001B[1;32m     33\u001B[0m     sys\u001B[38;5;241m.\u001B[39mmodules[mod\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m] \u001B[38;5;241m=\u001B[39m mod\n",
      "File \u001B[0;32m/tmp/torchinductor_root/hf/chf6mf5xkwhnwl5plgq5l52wrboju23rkhqdwvvhfuyafsdtdhcs.py:584\u001B[0m\n\u001B[1;32m    548\u001B[0m \u001B[38;5;66;03m# kernel path: /tmp/torchinductor_root/xx/cxxzxelsf4gi7a5kffdccxgqsfhminixo4cntvatfalen75dr7ug.py\u001B[39;00m\n\u001B[1;32m    549\u001B[0m \u001B[38;5;66;03m# Topologically Sorted Source Nodes: [mul], Original ATen: [aten.mul]\u001B[39;00m\n\u001B[1;32m    550\u001B[0m \u001B[38;5;66;03m# Source node to ATen node mapping:\u001B[39;00m\n\u001B[1;32m    551\u001B[0m \u001B[38;5;66;03m#   mul => mul\u001B[39;00m\n\u001B[1;32m    552\u001B[0m \u001B[38;5;66;03m# Graph fragment:\u001B[39;00m\n\u001B[1;32m    553\u001B[0m \u001B[38;5;66;03m#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_1, 0.6931471805599453), kwargs = {})\u001B[39;00m\n\u001B[1;32m    554\u001B[0m triton_poi_fused_mul_1 \u001B[38;5;241m=\u001B[39m async_compile\u001B[38;5;241m.\u001B[39mtriton(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtriton_poi_fused_mul_1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;124mimport triton\u001B[39m\n\u001B[1;32m    556\u001B[0m \u001B[38;5;124mimport triton.language as tl\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    580\u001B[0m \u001B[38;5;124m    tl.store(out_ptr0 + (x0), tmp2, xmask)\u001B[39m\n\u001B[1;32m    581\u001B[0m \u001B[38;5;124m'''\u001B[39m, device_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 584\u001B[0m \u001B[43masync_compile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    585\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m async_compile\n\u001B[1;32m    587\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcall\u001B[39m(args):\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/async_compile.py:491\u001B[0m, in \u001B[0;36mAsyncCompile.wait\u001B[0;34m(self, scope)\u001B[0m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m get_compile_threads() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    484\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m dynamo_timed(\n\u001B[1;32m    485\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masync_compile.wait\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    486\u001B[0m         log_pt2_compile_event\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    489\u001B[0m         waitcounter_name_override\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompile_triton\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    490\u001B[0m     ):\n\u001B[0;32m--> 491\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_futures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    493\u001B[0m _compile_end()\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/async_compile.py:511\u001B[0m, in \u001B[0;36mAsyncCompile._wait_futures\u001B[0;34m(self, scope)\u001B[0m\n\u001B[1;32m    509\u001B[0m     pbar\u001B[38;5;241m.\u001B[39mset_postfix_str(key)\n\u001B[1;32m    510\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m     kernel \u001B[38;5;241m=\u001B[39m \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    512\u001B[0m     scope[key] \u001B[38;5;241m=\u001B[39m kernel\n\u001B[1;32m    513\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m BrokenProcessPool \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/codecache.py:4014\u001B[0m, in \u001B[0;36mLambdaFuture.result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   4013\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mresult\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Callable[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, Any]:\n\u001B[0;32m-> 4014\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/async_compile.py:376\u001B[0m, in \u001B[0;36mAsyncCompile.triton.<locals>.get_result\u001B[0;34m()\u001B[0m\n\u001B[1;32m    373\u001B[0m kernel\u001B[38;5;241m.\u001B[39mset_compile_info(compile_id, is_backward)\n\u001B[1;32m    374\u001B[0m CompiledTritonKernels\u001B[38;5;241m.\u001B[39mremove_future(source_code)\n\u001B[0;32m--> 376\u001B[0m \u001B[43mkernel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprecompile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwarm_cache_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreload_kernel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreload_kernel_in_parent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstatic_triton_bundle_key\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCompiledTritonKernels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkey\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource_code\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    381\u001B[0m info \u001B[38;5;241m=\u001B[39m kernel\u001B[38;5;241m.\u001B[39mautotune_cache_info \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m    382\u001B[0m info[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompile_time_us\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m elapsed_us\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py:411\u001B[0m, in \u001B[0;36mCachingAutotuner.precompile\u001B[0;34m(self, warm_cache_only, reload_kernel, static_triton_bundle_key)\u001B[0m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m static_triton_bundle_key \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_statically_launchable():\n\u001B[1;32m    410\u001B[0m     TritonBundler\u001B[38;5;241m.\u001B[39mput_static_autotuner(static_triton_bundle_key, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 411\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_launchers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dynamic_scale_rblock()\n",
      "File \u001B[0;32m/venv/goedelv2/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py:573\u001B[0m, in \u001B[0;36mCachingAutotuner._make_launchers\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    571\u001B[0m             exc \u001B[38;5;241m=\u001B[39m e\n\u001B[1;32m    572\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(launchers) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 573\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo valid triton configs. \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(exc)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    574\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers \u001B[38;5;241m=\u001B[39m launchers\n",
      "\u001B[0;31mInductorError\u001B[0m: RuntimeError: No valid triton configs. OutOfMemoryError: out of resource: triton_tem_fused_0 Required: 107008 Hardware limit:101376 Reducing block sizes or `num_stages` may help.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "out.attentions[0][0][2]",
   "id": "7eaa8475cd6d0a9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# assert attentions are 0 at attn_mask, and nonzero elsewhere\n",
    "\n",
    "\n",
    "# new_mask = attn_mask.expand(-1, logits.attentions[0].shape[1], -1,-1)\n",
    "#\n",
    "# for layer in logits.attentions:\n",
    "#     for x_ in layer:\n",
    "#         for attns in x_:\n",
    "#             print (attns)\n",
    "#             break\n",
    "\n",
    "# comparing to attn_mask, looks like the attentions are correctly working. I.e. attentions are 0 for masked, and non-zero elsewhere\n",
    "# Interestingly, high attention values often found for subsequent tokens even though model only used to seeing past tokens.\n",
    "# Suggests annealing worthwhile to limit influence earlier in adaptation process"
   ],
   "id": "f9e2ddcf8ad3a982",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo:\n",
    "# - set up dataset, test edit flow data prep code (couplings, alignments etc. ).\n",
    "# - Look at generating from empty coupling as well as error correcting coupling (i.e. add error to context, modify the code directly)\n",
    "# - Set up model wrapper (rate predictions etc.), Quantization/LoRA. Unlike MDM adaptation, we can probably keep the logits unshifted, as we are predicting both substitute and insert next token probs, as well as delete.\n"
   ],
   "id": "590bf73c9818134f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
